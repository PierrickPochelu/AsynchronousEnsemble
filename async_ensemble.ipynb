{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qsu_5zzyp8ub"
      },
      "source": [
        "#Asynchronous ensembles\n",
        "\n",
        "Different simple asynchronous ensemble designs are compared.\n",
        "\n",
        "\n",
        "Given `n` processes (predicting on CIFAR10 images), `m` devices (CPU, GPU, or TPU), and the given assignment map `n[i]->m[j]`. We evaluate different design strategies: \n",
        "* Strategy \"Sequential\". It consists of iteratively (for-loop) each base model one by one, then their predictions are averaged.\n",
        "* Strategy \"Multiprocessing\". It consists of Multiprocessing is a built-in python package to run multiple processes in parallel. Here, we launch multiple TF sessions in parallel for each base model.\n",
        "* Strategy \"1 TF session\". It consists in combining base models in one single Tensorflow session.\n",
        "\n",
        "\n",
        "ML tasks may interfere with each other negatively and experience performance unpredictability. This is due the parallel tasks share resources: core utilization, core caches, IO, buses.\n",
        "\n",
        "<!--\n",
        "Some more general papers [[1]](#1) [[2]](#2) exist to efficiently assign `n` heterogeneous AI models on `m` devices at inference time but do not provide code. They focus on the combinatorial optimization problem. Here, the assignment map `n[i]->m[j]` is given. Another axis of works consists in compiling/optimizing neural networks but not shown here.\n",
        "\n",
        "<a id=\"1\">[1]</a>  Automatic assignement based on RL: https://i2.cs.hku.hk/~cwu/papers/yxbao-infocom19.pdf\n",
        "\n",
        "<a id=\"2\">[2]</a>  Automatic assignement and tuning (batch size, data-parallel) processes:  https://arxiv.org/pdf/2208.14049.pdf\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bYwVPjsqFik"
      },
      "source": [
        "## Common settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "QqboavAvptON",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30cc5cc5-ef1f-4c72-de11-a9df809e5a1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Common imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras import datasets, models\n",
        "from tensorflow.keras import Input, Model, optimizers, layers\n",
        "from tensorflow.keras.layers import (\n",
        "    Conv2D,\n",
        "    Activation,\n",
        "    MaxPooling2D,\n",
        "    Dropout,\n",
        "    Flatten,\n",
        "    Dense,\n",
        ")\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Common constants\n",
        "NB_TRAINING_SAMPLES = 6000  # Number training samples for faster experiences\n",
        "NB_INFERENCE_SAMPLES = 6000  # Number inference samples\n",
        "ENSEMBLE_SIZE = 4\n",
        "NB_EPOCHS = 1\n",
        "GPUID = [0, 0, 0, 0]  #  # asignement of models with GPU ID or -1\n",
        "BATCH_SIZE = 4\n",
        "\n",
        "# Common base model\n",
        "def keras_model(x):\n",
        "    x = layers.Conv2D(32, (3, 3), activation=\"relu\")(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation=\"relu\")(x)\n",
        "    x = layers.MaxPooling2D((2, 2))(x)\n",
        "    x = layers.Conv2D(64, (3, 3), activation=\"relu\")(x)\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = layers.Dense(10, activation=\"softmax\")(x)\n",
        "    x = layers.Softmax()(x)\n",
        "    return x\n",
        "\n",
        "# Common evaluation procedure\n",
        "def evaluate_model(model, x, y):\n",
        "    start_time = time.time()\n",
        "    y_pred = model.predict(x, batch_size=BATCH_SIZE) # <--- inference is here\n",
        "    enlapsed = time.time() - start_time\n",
        "    acc = np.mean(np.argmax(y, axis=1) == np.argmax(y_pred, axis=1))\n",
        "    return {\"accuracy\": round(acc, 2), \"time\": round(enlapsed, 2)}\n",
        "  \n",
        "# Common data\n",
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)\n",
        "train_images = train_images[:NB_TRAINING_SAMPLES]\n",
        "train_labels = train_labels[:NB_TRAINING_SAMPLES]\n",
        "test_images = test_images[:NB_INFERENCE_SAMPLES]\n",
        "test_labels = test_labels[:NB_INFERENCE_SAMPLES]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP2hf1Y2MHyf"
      },
      "source": [
        "##Strategy \"Sequential\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bulr9xG2MRlR",
        "outputId": "8edd7f3c-fc5a-4822-ae17-5e9d301dcaab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500/1500 [==============================] - 19s 11ms/step - loss: 2.2526\n",
            "1500/1500 [==============================] - 18s 11ms/step - loss: 2.2707\n",
            "1500/1500 [==============================] - 19s 12ms/step - loss: 2.3430\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 2.2494\n",
            "1500/1500 [==============================] - 6s 4ms/step\n",
            "Model id: 0 accuracy: 0.24 time: 6.34\n",
            "1500/1500 [==============================] - 6s 4ms/step\n",
            "Model id: 1 accuracy: 0.2 time: 6.43\n",
            "1500/1500 [==============================] - 6s 4ms/step\n",
            "Model id: 2 accuracy: 0.1 time: 6.84\n",
            "1500/1500 [==============================] - 7s 4ms/step\n",
            "Model id: 3 accuracy: 0.26 time: 10.41\n",
            "1500/1500 [==============================] - 8s 5ms/step\n",
            "1500/1500 [==============================] - 6s 4ms/step\n",
            "1500/1500 [==============================] - 6s 4ms/step\n",
            "1500/1500 [==============================] - 6s 4ms/step\n",
            "Ensemble accuracy: 0.24 inference time: 31.84\n"
          ]
        }
      ],
      "source": [
        "def keras_model_builder(config):\n",
        "    input_shape = (32, 32, 3)\n",
        "    loss = \"categorical_crossentropy\"\n",
        "    opt = \"adam\"\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config[\"gpuid\"])\n",
        "    os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
        "    input = Input(shape=input_shape)\n",
        "    output = keras_model(input)\n",
        "    model = Model(inputs=input, outputs=output)\n",
        "    model.compile(loss=loss, optimizer=opt)\n",
        "    return model\n",
        "def gen_pred(models, x, batch_size):\n",
        "  for model_i in models:\n",
        "    yield model_i.predict(x, batch_size=batch_size)\n",
        "\n",
        "class ensemble:\n",
        "    def __init__(self, ensemble_size, gpus):\n",
        "        self.loss = \"categorical_crossentropy\"\n",
        "        self.opt = \"adam\"\n",
        "        self.ensemble_size=ensemble_size\n",
        "        self.gpus=gpus\n",
        "        self.models=[]\n",
        "        for i in range(ensemble_size):\n",
        "            config_i={\"gpuid\":self.gpus[i]}\n",
        "            model_i=keras_model_builder(config_i)\n",
        "            model_i.compile(loss=self.loss, optimizer=self.opt)\n",
        "            self.models.append(model_i)\n",
        "\n",
        "    def fit(self, train_images, train_labels):\n",
        "        for model_i in self.models:\n",
        "            model_i.fit(\n",
        "                x=train_images, \n",
        "                y=train_labels, \n",
        "                batch_size=BATCH_SIZE, \n",
        "                epochs=NB_EPOCHS)\n",
        "\n",
        "    def predict(self, x, batch_size):\n",
        "        cumulated_preds=None\n",
        "        g=gen_pred(self.models, x, batch_size)\n",
        "        for preds_i in g:\n",
        "          if cumulated_preds is None:\n",
        "            cumulated_preds=preds_i\n",
        "          else:\n",
        "            cumulated_preds+=preds_i\n",
        "        return cumulated_preds\n",
        "\n",
        "# Training\n",
        "ensemble = ensemble(ensemble_size=ENSEMBLE_SIZE, gpus=GPUID)\n",
        "ensemble.fit(train_images, train_labels)\n",
        "\n",
        "# Inference\n",
        "for i, base_model in enumerate(ensemble.models):\n",
        "    info = evaluate_model(base_model, test_images, test_labels)\n",
        "    print(f\"Model id: {i} accuracy: {info['accuracy']} time: {info['time']}\")\n",
        "\n",
        "info = evaluate_model(ensemble, test_images, test_labels)\n",
        "print(f\"Ensemble accuracy: {info['accuracy']} inference time: {info['time']}\")\n",
        "\n",
        "# Destroy\n",
        "del ensemble\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yN9FsPs5Ch5h"
      },
      "source": [
        "##Strategy \"Multiprocessing\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrpF_0XHpZS9",
        "outputId": "e48c1852-58e0-4cfb-f2e3-fd5d1f609e3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The building/training is launched ...\n",
            "1500/1500 [==============================] - 76s 49ms/step - loss: 2.2539\n",
            "1500/1500 [==============================] - 77s 49ms/step - loss: 2.2543\n",
            "1500/1500 [==============================] - 78s 49ms/step - loss: 2.2505\n",
            "1500/1500 [==============================] - 78s 50ms/step - loss: 2.2588\n",
            "1500/1500 [==============================] - 30s 20ms/step\n",
            "1500/1500 [==============================] - 29s 19ms/step\n",
            "1500/1500 [==============================] - 30s 19ms/step\n",
            "1500/1500 [==============================] - 30s 20ms/step\n",
            "Model rank: 2 accuracy: 0.28 time: 32.43\n",
            "Model rank: 1 accuracy: 0.23 time: 42.42\n",
            "Model rank: 0 accuracy: 0.1 time: 41.94\n",
            "Model rank: 3 accuracy: 0.1 time: 42.05\n",
            "Ensemble accuracy: 0.1 inference time: 21.72\n"
          ]
        }
      ],
      "source": [
        "from multiprocessing import Queue, Process\n",
        "import math\n",
        "SEGSIZE=500\n",
        "\n",
        "def keras_model_builder(config):\n",
        "    input_shape = (32, 32, 3)\n",
        "    loss = \"categorical_crossentropy\"\n",
        "    opt = \"adam\"\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config[\"gpuid\"])\n",
        "    os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"] = \"true\"\n",
        "    input = Input(shape=input_shape)\n",
        "    output = keras_model(input)\n",
        "    model = Model(inputs=input, outputs=output)\n",
        "    model.compile(loss=loss, optimizer=opt)\n",
        "    return model\n",
        "\n",
        "class MyProcess(Process):\n",
        "    def __init__(\n",
        "        self,\n",
        "        rank: int,\n",
        "        config: dict,\n",
        "        dataset: list,\n",
        "        shared_input_queue: Queue,\n",
        "        shared_output_queue: Queue,\n",
        "    ): \n",
        "        # WARNING: don't use global variable in MyProcess \n",
        "        # otherwise -> unexpected behaviour and deadlock may happen\n",
        "        Process.__init__(self, name=\"ModelProcessor\")\n",
        "        self.rank = rank\n",
        "        self.config = config\n",
        "        self.dataset = dataset  # List of np.ndarray xtrain, ytrain, xtest, ytest\n",
        "        self.shared_input_queue = shared_input_queue  # 'go' or 'stop'\n",
        "        self.shared_output_queue = shared_output_queue  # \"initok\" or predictions\n",
        "        self.model = None\n",
        "        self.info = None\n",
        "\n",
        "    def _asynchronous_predict(self):\n",
        "        finish = False\n",
        "        def generator():\n",
        "          x=self.dataset[2]\n",
        "          for segi in range(0,len(x),self.config[\"SEGSIZE\"]):\n",
        "            segment_i=x[i:i+self.config[\"SEGSIZE\"]]\n",
        "            segout = self.model.predict(segment_i, \n",
        "                                        batch_size=BATCH_SIZE, \n",
        "                                        verbose=0)\n",
        "            yield segi, segout\n",
        "        gen=generator()\n",
        "        while finish == False:\n",
        "            msg = self.shared_input_queue.get()  # wait\n",
        "            if msg == \"go\":\n",
        "                for segi, segout in gen:\n",
        "                  self.shared_output_queue.put((self.rank, segi, segout))\n",
        "            else:\n",
        "                finish = True\n",
        "\n",
        "    def train(self):\n",
        "        self.model.fit(\n",
        "            x=self.dataset[0],\n",
        "            y=self.dataset[1],\n",
        "            batch_size=self.config[\"batch_size\"],\n",
        "            epochs=self.config[\"epochs\"],\n",
        "        )\n",
        "\n",
        "    def run(self):\n",
        "        self.model = keras_model_builder(self.config)\n",
        "        self.train()\n",
        "        info = evaluate_model(self.model, x=self.dataset[2], y=self.dataset[3])\n",
        "        self.shared_output_queue.put((self.rank, -1, info))  # notify the main process\n",
        "\n",
        "        self._asynchronous_predict()  # run forever\n",
        "\n",
        "############\n",
        "# TRAINING #\n",
        "############\n",
        "input_queue = Queue()\n",
        "output_queue = Queue()\n",
        "processes = []\n",
        "\n",
        "\n",
        "# Launch training process\n",
        "for i in range(ENSEMBLE_SIZE):\n",
        "    proc = MyProcess(\n",
        "        rank=i,\n",
        "        config={\"gpuid\": GPUID[i], \"epochs\": NB_EPOCHS, \n",
        "                \"batch_size\": BATCH_SIZE, \"SEGSIZE\": SEGSIZE},\n",
        "        dataset=[train_images, train_labels, test_images, test_labels],\n",
        "        shared_input_queue=input_queue,\n",
        "        shared_output_queue=output_queue,\n",
        "    )\n",
        "    proc.start()  # start and wait\n",
        "    processes.append(proc)\n",
        "print(\"The building/training is launched ...\")\n",
        "\n",
        "# Wait every processes is ready\n",
        "training_start_time = time.time()\n",
        "for i in range(ENSEMBLE_SIZE):\n",
        "    thread_id, seg_id, msg = output_queue.get()\n",
        "    if isinstance(msg, dict):\n",
        "        print(\n",
        "            f\"Model rank: {thread_id} accuracy: {msg['accuracy']} time: {msg['time']}\"\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"thread {thread_id} received an unexpected message\")\n",
        "\n",
        "#############\n",
        "# Inference #\n",
        "#############\n",
        "def gen_read(processes, output_queue):\n",
        "    nbreq=math.ceil(float(len(test_images))/SEGSIZE) * len(processes)\n",
        "    for i in range(nbreq):\n",
        "      thread_id, segi, segout = output_queue.get()\n",
        "      yield segi, segout\n",
        "preds = np.zeros(test_labels.shape, np.float32)\n",
        "gen=gen_read(processes, output_queue)\n",
        "\n",
        "inference_start_time = time.time()\n",
        "for process in processes:\n",
        "    input_queue.put(\"go\")\n",
        "\n",
        "for segi, segout in gen:\n",
        "    start=segi\n",
        "    end=min( segi+len(segout) , len(preds) )\n",
        "    preds[start:end] = segout + preds[start:end]\n",
        "\n",
        "inf_time = time.time() - inference_start_time\n",
        "acc = np.mean(np.argmax(test_labels, axis=1) == np.argmax(preds, axis=1))\n",
        "print(f\"Ensemble accuracy: {round(acc,2)} inference time: {round(inf_time,2)}\")\n",
        "\n",
        "# Stop processes\n",
        "for i in range(ENSEMBLE_SIZE):\n",
        "    input_queue.put(\"stop\")\n",
        "del processes\n",
        "tf.keras.backend.clear_session()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aZ5P8zKHAwI"
      },
      "source": [
        "##Strategy \"1 TF Session\"\n",
        "."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AmjB8a9fpqba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae3c6b46-f53b-490e-b0bd-56d35decbbfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500/1500 [==============================] - 16s 11ms/step - loss: 2.2504\n",
            "1500/1500 [==============================] - 16s 11ms/step - loss: 2.2625\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 2.2632\n",
            "1500/1500 [==============================] - 17s 11ms/step - loss: 2.3295\n",
            "1500/1500 [==============================] - 6s 4ms/step\n",
            "Model id: 0 accuracy: 0.25 time: 6.81\n",
            "1500/1500 [==============================] - 6s 4ms/step\n",
            "Model id: 1 accuracy: 0.1 time: 6.51\n",
            "1500/1500 [==============================] - 6s 4ms/step\n",
            "Model id: 2 accuracy: 0.21 time: 10.43\n",
            "1500/1500 [==============================] - 6s 4ms/step\n",
            "Model id: 3 accuracy: 0.1 time: 10.38\n",
            "1500/1500 [==============================] - 15s 10ms/step\n",
            "Ensemble accuracy: 0.19 inference time: 15.02\n"
          ]
        }
      ],
      "source": [
        "def from_gpu_id_to_device_name(gpuid):\n",
        "    if gpuid == -1:\n",
        "        return \"/device:CPU:0\"\n",
        "    else:\n",
        "        return \"/device:GPU:\" + str(gpuid)\n",
        "\n",
        "\n",
        "class ensemble:\n",
        "    def __init__(self, ensemble_size, gpus):\n",
        "        self.loss = \"categorical_crossentropy\"\n",
        "        self.opt = \"adam\"\n",
        "\n",
        "        self.model_list = []\n",
        "        output_list = []\n",
        "\n",
        "        with tf.device(from_gpu_id_to_device_name(gpus[0])):\n",
        "            input = Input(shape=(32, 32, 3))\n",
        "\n",
        "        for i in range(ensemble_size):\n",
        "            with tf.device(from_gpu_id_to_device_name(gpus[i])):\n",
        "                input_i = tf.identity(input)\n",
        "                output_i = keras_model(input_i)\n",
        "                model_i = Model(inputs=input_i, outputs=output_i)\n",
        "                output_list.append(output_i)\n",
        "                self.model_list.append(model_i)\n",
        "\n",
        "        with tf.device(from_gpu_id_to_device_name(gpus[0])):\n",
        "            merge = tf.stack(output_list, axis=-1)\n",
        "            combined_predictions = tf.reduce_mean(merge, axis=-1)\n",
        "\n",
        "        self.ensemble = Model(inputs=input, outputs=combined_predictions)\n",
        "        self.ensemble.compile(loss=self.loss, optimizer=self.opt)\n",
        "\n",
        "    def fit(self, train_images, train_labels):\n",
        "        for model_i in self.model_list:\n",
        "            model_i.compile(loss=self.loss, optimizer=self.opt)\n",
        "            model_i.fit(\n",
        "                x=train_images, y=train_labels, \n",
        "                batch_size=BATCH_SIZE, epochs=NB_EPOCHS)\n",
        "\n",
        "    def predict(self, x, batch_size):\n",
        "        return self.ensemble.predict(x, batch_size=batch_size)\n",
        "\n",
        "# Training\n",
        "ensemble = ensemble(ensemble_size=ENSEMBLE_SIZE, gpus=GPUID)\n",
        "ensemble.fit(train_images, train_labels)\n",
        "\n",
        "# Inference\n",
        "for i, base_model in enumerate(ensemble.model_list):\n",
        "    info = evaluate_model(base_model, test_images, test_labels)\n",
        "    print(f\"Model id: {i} accuracy: {info['accuracy']} time: {info['time']}\")\n",
        "\n",
        "info = evaluate_model(ensemble, test_images, test_labels)\n",
        "print(f\"Ensemble accuracy: {info['accuracy']} inference time: {info['time']}\")\n",
        "\n",
        "# Destroy\n",
        "del ensemble\n",
        "tf.keras.backend.clear_session()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePRwRKgHGYuK"
      },
      "source": [
        "##Experimental results\n",
        "\n",
        "Results obtained with Google Colab.\n",
        " \n",
        "| Device | Iterative design | Multiprocessing design | 1 TF Session design |\n",
        "| ------ | ------ | ------ | ------ |\n",
        "| GPU | 16.33 sec | 9.68 sec | 4.65 sec |\n",
        "| TPU | 36.37 sec | 25.98 sec | 20.78 sec |\n",
        "| CPU | 31.84 sec | 21.72 sec | 15.02 sec |\n",
        "\n",
        "##Conclusion \n",
        "The asynchronous ensemble designs exploits the underlying parallelism. \n",
        "\n",
        "Multiprocessing design requires each process know the data. Thus, memory consumption for data and buses communication are multiplied. In addition of that, 1 Tensorflow session may optimize more efficiently the assignment of tensors and cores than independant sessions. However, multiple independant process may allow more flexibility for ML applications, like predicting with different inference frameworks (e.g., Torch, Tensorflow, JAX, ...).\n",
        "\n",
        "We may expect the results vary according models size, hardware, ... MIG technology (multi-instances) may offer better performance and deterministic performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_lSAORZiRymT"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
